2020-03-22 12:35:10,876 | INFO | Brief summary of experiment : 

    Train jointly the AutoEncoder and the DeepSAD model following Lukas Ruff et
    al. (2019) work adapted to the MURA dataset (preprocessing inspired from the
    work of Davletshina et al. (2020)). The network structure is a ResNet18
    AutoEncoder until the Adaptative average pooling layer. The AE embdedding is
    thus (512, 16, 16). This embdedding is further processed through 3 convolutional
    layers (specific to the SVDD embdedding generation) to provide the SVDD
    embdedding of 512. The network is trained with two loss functions: a masked MSE
    loss for the reconstruction and the DeepSAD loss on the embedding. The two
    losses are scaled to be comparable by perfoming one forward pass prior the
    training. The Encoder is not initialized with weights trained on ImageNet.
    The AE masked reconstruction loss is not computed for known abnormal sample
    so that the AE learn to reconstruct normal samples only. The network input is
    masked with the mask : only the body part is kept and the background is set
    to zero. The AE is pretrained over 5 epochs in order to improve the initialization
    of the hypersphere center (we hypothetize that with a pretrained AE the
    hypersphere center estimation will be more meaningful). Note that the 'affine'
    parameters of BatchNorm2d layers has been changed to False in this implementation.
    
2020-03-22 12:35:10,876 | INFO | Log file : ../../../Outputs/JointDeepSAD_2020_03_22_00h21/logs/log_2.txt
2020-03-22 12:35:10,877 | INFO | Data path : ../../../data/PROCESSED/
2020-03-22 12:35:10,877 | INFO | Outputs path : ../../../Outputs/JointDeepSAD_2020_03_22_00h21/

2020-03-22 12:35:14,955 | INFO | Train fraction : 50%
2020-03-22 12:35:14,956 | INFO | Fraction knonw normal : 5%
2020-03-22 12:35:14,956 | INFO | Fraction known abnormal : 5%
2020-03-22 12:35:14,988 | INFO | Split Summary 
+-------+----------------+------------+--------------+
| Set   | Name           | Number [-] | Fraction [%] |
+-------+----------------+------------+--------------+
| train | Normal         | 18033      | 94.73%       |
| train | Abnormal       | 1004       | 5.27%        |
| train | Normal known   | 961        | 5.05%        |
| train | Abnormal known | 1004       | 5.27%        |
| train | Unknown        | 17072      | 89.68%       |
| ----  | ----           | ----       | ----         |
| valid | Normal         | 2772       | 26.66%       |
| valid | Abnormal       | 7627       | 73.34%       |
| valid | Normal known   | 489        | 4.70%        |
| valid | Abnormal known | 506        | 4.87%        |
| valid | Unknown        | 9404       | 90.43%       |
| ----  | ----           | ----       | ----         |
| test  | Normal         | 2763       | 26.29%       |
| test  | Abnormal       | 7748       | 73.71%       |
| test  | Normal known   | 506        | 4.81%        |
| test  | Abnormal known | 498        | 4.74%        |
| test  | Unknown        | 9507       | 90.45%       |
+-------+----------------+------------+--------------+
2020-03-22 12:35:14,988 | INFO | Online preprocessing pipeline : 
             Grayscale()              
                  |                   
                  V                   
        AutoContrast(cutoff=1)        
                  |                   
                  V                   
     RandomHorizontalFlip(p=0.5)      
                  |                   
                  V                   
      RandomVerticalFlip(p=0.5)       
                  |                   
                  V                   
RandomBrightness(lower=0.8, upper=1.2)
                  |                   
                  V                   
RandomScaling(scale_range=(0.8, 1.2)) 
                  |                   
                  V                   
RandomRotation(degree_range=(-20, 20))
                  |                   
                  V                   
        ResizeMax(max_len=512)        
                  |                   
                  V                   
            PadToSquare()             
                  |                   
                  V                   
 MinMaxNormalization(vmin=0, vmax=1)  
                  |                   
                  V                   
           ToTorchTensor()            

2020-03-22 12:35:14,988 | INFO | Set seed 02/02 to 1111
2020-03-22 12:35:14,988 | INFO | Device : cuda
2020-03-22 12:35:14,988 | INFO | Number of thread : 0
2020-03-22 12:35:14,988 | INFO | Number of dataloader worker for Joint DeepSAD : 8

2020-03-22 12:35:15,199 | INFO | Network : AE_SVDD_Hybrid
2020-03-22 12:35:15,199 | INFO | Autoencoder pretrained on ImageNet : False
2020-03-22 12:35:15,199 | INFO | DeepSAD eta : 1.0
2020-03-22 12:35:15,228 | INFO | Network architecture: 
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [16, 64, 256, 256]           9,408
       BatchNorm2d-2         [16, 64, 256, 256]               0
              ReLU-3         [16, 64, 256, 256]               0
         MaxPool2d-4         [16, 64, 128, 128]               0
            Conv2d-5         [16, 64, 128, 128]          36,864
       BatchNorm2d-6         [16, 64, 128, 128]               0
              ReLU-7         [16, 64, 128, 128]               0
            Conv2d-8         [16, 64, 128, 128]          36,864
       BatchNorm2d-9         [16, 64, 128, 128]               0
             ReLU-10         [16, 64, 128, 128]               0
     DownResBlock-11         [16, 64, 128, 128]               0
           Conv2d-12         [16, 64, 128, 128]          36,864
      BatchNorm2d-13         [16, 64, 128, 128]               0
             ReLU-14         [16, 64, 128, 128]               0
           Conv2d-15         [16, 64, 128, 128]          36,864
      BatchNorm2d-16         [16, 64, 128, 128]               0
             ReLU-17         [16, 64, 128, 128]               0
     DownResBlock-18         [16, 64, 128, 128]               0
           Conv2d-19          [16, 128, 64, 64]           8,192
      BatchNorm2d-20          [16, 128, 64, 64]               0
           Conv2d-21          [16, 128, 64, 64]          73,728
      BatchNorm2d-22          [16, 128, 64, 64]               0
             ReLU-23          [16, 128, 64, 64]               0
           Conv2d-24          [16, 128, 64, 64]         147,456
      BatchNorm2d-25          [16, 128, 64, 64]               0
             ReLU-26          [16, 128, 64, 64]               0
     DownResBlock-27          [16, 128, 64, 64]               0
           Conv2d-28          [16, 128, 64, 64]         147,456
      BatchNorm2d-29          [16, 128, 64, 64]               0
             ReLU-30          [16, 128, 64, 64]               0
           Conv2d-31          [16, 128, 64, 64]         147,456
      BatchNorm2d-32          [16, 128, 64, 64]               0
             ReLU-33          [16, 128, 64, 64]               0
     DownResBlock-34          [16, 128, 64, 64]               0
           Conv2d-35          [16, 256, 32, 32]          32,768
      BatchNorm2d-36          [16, 256, 32, 32]               0
           Conv2d-37          [16, 256, 32, 32]         294,912
      BatchNorm2d-38          [16, 256, 32, 32]               0
             ReLU-39          [16, 256, 32, 32]               0
           Conv2d-40          [16, 256, 32, 32]         589,824
      BatchNorm2d-41          [16, 256, 32, 32]               0
             ReLU-42          [16, 256, 32, 32]               0
     DownResBlock-43          [16, 256, 32, 32]               0
           Conv2d-44          [16, 256, 32, 32]         589,824
      BatchNorm2d-45          [16, 256, 32, 32]               0
             ReLU-46          [16, 256, 32, 32]               0
           Conv2d-47          [16, 256, 32, 32]         589,824
      BatchNorm2d-48          [16, 256, 32, 32]               0
             ReLU-49          [16, 256, 32, 32]               0
     DownResBlock-50          [16, 256, 32, 32]               0
           Conv2d-51          [16, 512, 16, 16]         131,072
      BatchNorm2d-52          [16, 512, 16, 16]               0
           Conv2d-53          [16, 512, 16, 16]       1,179,648
      BatchNorm2d-54          [16, 512, 16, 16]               0
             ReLU-55          [16, 512, 16, 16]               0
           Conv2d-56          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-57          [16, 512, 16, 16]               0
             ReLU-58          [16, 512, 16, 16]               0
     DownResBlock-59          [16, 512, 16, 16]               0
           Conv2d-60          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-61          [16, 512, 16, 16]               0
             ReLU-62          [16, 512, 16, 16]               0
           Conv2d-63          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-64          [16, 512, 16, 16]               0
             ReLU-65          [16, 512, 16, 16]               0
     DownResBlock-66          [16, 512, 16, 16]               0
 ResNet18_Encoder-67          [16, 512, 16, 16]               0
           Conv2d-68          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-69          [16, 512, 16, 16]               0
             ReLU-70          [16, 512, 16, 16]               0
           Conv2d-71          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-72          [16, 512, 16, 16]               0
             ReLU-73          [16, 512, 16, 16]               0
       UpResBlock-74          [16, 512, 16, 16]               0
           Conv2d-75          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-76          [16, 512, 16, 16]               0
             ReLU-77          [16, 512, 16, 16]               0
         Upsample-78          [16, 512, 32, 32]               0
           Conv2d-79          [16, 256, 32, 32]         131,072
      BatchNorm2d-80          [16, 256, 32, 32]               0
         Upsample-81          [16, 512, 32, 32]               0
           Conv2d-82          [16, 256, 32, 32]         131,072
      BatchNorm2d-83          [16, 256, 32, 32]               0
             ReLU-84          [16, 256, 32, 32]               0
       UpResBlock-85          [16, 256, 32, 32]               0
           Conv2d-86          [16, 256, 32, 32]         589,824
      BatchNorm2d-87          [16, 256, 32, 32]               0
             ReLU-88          [16, 256, 32, 32]               0
           Conv2d-89          [16, 256, 32, 32]         589,824
      BatchNorm2d-90          [16, 256, 32, 32]               0
             ReLU-91          [16, 256, 32, 32]               0
       UpResBlock-92          [16, 256, 32, 32]               0
           Conv2d-93          [16, 256, 32, 32]         589,824
      BatchNorm2d-94          [16, 256, 32, 32]               0
             ReLU-95          [16, 256, 32, 32]               0
         Upsample-96          [16, 256, 64, 64]               0
           Conv2d-97          [16, 128, 64, 64]          32,768
      BatchNorm2d-98          [16, 128, 64, 64]               0
         Upsample-99          [16, 256, 64, 64]               0
          Conv2d-100          [16, 128, 64, 64]          32,768
     BatchNorm2d-101          [16, 128, 64, 64]               0
            ReLU-102          [16, 128, 64, 64]               0
      UpResBlock-103          [16, 128, 64, 64]               0
          Conv2d-104          [16, 128, 64, 64]         147,456
     BatchNorm2d-105          [16, 128, 64, 64]               0
            ReLU-106          [16, 128, 64, 64]               0
          Conv2d-107          [16, 128, 64, 64]         147,456
     BatchNorm2d-108          [16, 128, 64, 64]               0
            ReLU-109          [16, 128, 64, 64]               0
      UpResBlock-110          [16, 128, 64, 64]               0
          Conv2d-111          [16, 128, 64, 64]         147,456
     BatchNorm2d-112          [16, 128, 64, 64]               0
            ReLU-113          [16, 128, 64, 64]               0
        Upsample-114        [16, 128, 128, 128]               0
          Conv2d-115         [16, 64, 128, 128]           8,192
     BatchNorm2d-116         [16, 64, 128, 128]               0
        Upsample-117        [16, 128, 128, 128]               0
          Conv2d-118         [16, 64, 128, 128]           8,192
     BatchNorm2d-119         [16, 64, 128, 128]               0
            ReLU-120         [16, 64, 128, 128]               0
      UpResBlock-121         [16, 64, 128, 128]               0
          Conv2d-122         [16, 64, 128, 128]          36,864
     BatchNorm2d-123         [16, 64, 128, 128]               0
            ReLU-124         [16, 64, 128, 128]               0
          Conv2d-125         [16, 64, 128, 128]          36,864
     BatchNorm2d-126         [16, 64, 128, 128]               0
            ReLU-127         [16, 64, 128, 128]               0
      UpResBlock-128         [16, 64, 128, 128]               0
          Conv2d-129         [16, 64, 128, 128]          36,864
     BatchNorm2d-130         [16, 64, 128, 128]               0
            ReLU-131         [16, 64, 128, 128]               0
        Upsample-132         [16, 64, 256, 256]               0
          Conv2d-133         [16, 64, 256, 256]           4,096
     BatchNorm2d-134         [16, 64, 256, 256]               0
        Upsample-135         [16, 64, 256, 256]               0
          Conv2d-136         [16, 64, 256, 256]           4,096
     BatchNorm2d-137         [16, 64, 256, 256]               0
            ReLU-138         [16, 64, 256, 256]               0
      UpResBlock-139         [16, 64, 256, 256]               0
        Upsample-140         [16, 64, 512, 512]               0
          Conv2d-141          [16, 1, 512, 512]              64
            Tanh-142          [16, 1, 512, 512]               0
ResNet18_Decoder-143          [16, 1, 512, 512]               0
       AvgPool2d-144            [16, 512, 7, 7]               0
          Conv2d-145            [16, 256, 5, 5]       1,179,648
     BatchNorm2d-146            [16, 256, 5, 5]               0
            ReLU-147            [16, 256, 5, 5]               0
       MaxPool2d-148            [16, 256, 2, 2]               0
          Conv2d-149            [16, 128, 2, 2]          32,768
     BatchNorm2d-150            [16, 128, 2, 2]               0
================================================================
Total params: 22,131,968
Trainable params: 22,131,968
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 16.00
Forward/backward pass size (MB): 16149.66
Params size (MB): 84.43
Estimated Total Size (MB): 16250.08
----------------------------------------------------------------


2020-03-22 12:35:15,228 | INFO | Joint DeepSAD number of epoch : 100
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD number of pretraining epoch: 5
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD learning rate : 0.0001
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD learning rate milestone : [40, 80]
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD weight_decay : 1e-06
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD optimizer : Adam
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD batch_size 16
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD number of dataloader worker : 8
2020-03-22 12:35:15,228 | INFO | Joint DeepSAD criterion weighting : 0.6 Reconstruction loss + 0.4 SVDD embdedding loss

2020-03-22 12:35:15,230 | INFO | >>> Start Pretraining the Autoencoder.
2020-03-22 12:42:08,088 | INFO | | Epoch: 001/005 | Pretrain Time: 412.857 [s] | Pretrain Loss: 0.009491 |
2020-03-22 12:49:01,509 | INFO | | Epoch: 002/005 | Pretrain Time: 413.420 [s] | Pretrain Loss: 0.004798 |
2020-03-22 12:55:54,819 | INFO | | Epoch: 003/005 | Pretrain Time: 413.309 [s] | Pretrain Loss: 0.003810 |
2020-03-22 13:02:47,956 | INFO | | Epoch: 004/005 | Pretrain Time: 413.138 [s] | Pretrain Loss: 0.003216 |
2020-03-22 13:09:41,062 | INFO | | Epoch: 005/005 | Pretrain Time: 413.105 [s] | Pretrain Loss: 0.003161 |
2020-03-22 13:09:41,062 | INFO | >>> Pretraining of AutoEncoder Time: 2065.832 [s]
2020-03-22 13:09:41,062 | INFO | >>> Finished of AutoEncoder Pretraining.

2020-03-22 13:09:41,069 | INFO | >>> Initializing the hypersphere center.
2020-03-22 13:11:42,516 | INFO | >>> Center succesfully initialized.
2020-03-22 13:11:42,517 | INFO | >>> Initializing the loss scale factors.
2020-03-22 13:14:02,369 | INFO | >>> reconstruction loss scale factor initialized to 341.140830
2020-03-22 13:14:02,369 | INFO | >>> SVDD embdeding loss scale factor initialized to 0.001917
2020-03-22 13:14:02,374 | INFO | >>> Start Training the Joint DeepSAD and Autoencoder.
2020-03-22 13:14:02,375 | INFO | -----> LR = 0.0001
2020-03-22 13:20:57,074 | INFO | | Epoch: 001/100 | Train Time: 414.699 [s] | Train Loss: 0.950731 |
2020-03-22 13:20:57,075 | INFO | -----> LR = 0.0001
2020-03-22 13:27:51,945 | INFO | | Epoch: 002/100 | Train Time: 414.870 [s] | Train Loss: 0.826219 |
2020-03-22 13:27:51,945 | INFO | -----> LR = 0.0001
2020-03-22 13:34:46,675 | INFO | | Epoch: 003/100 | Train Time: 414.730 [s] | Train Loss: 0.783804 |
2020-03-22 13:34:46,675 | INFO | -----> LR = 0.0001
2020-03-22 13:41:41,471 | INFO | | Epoch: 004/100 | Train Time: 414.795 [s] | Train Loss: 0.728282 |
2020-03-22 13:41:41,471 | INFO | -----> LR = 0.0001
2020-03-22 13:48:36,410 | INFO | | Epoch: 005/100 | Train Time: 414.939 [s] | Train Loss: 0.702394 |
2020-03-22 13:48:36,410 | INFO | -----> LR = 0.0001
2020-03-22 13:55:31,191 | INFO | | Epoch: 006/100 | Train Time: 414.781 [s] | Train Loss: 0.678353 |
2020-03-22 13:55:31,192 | INFO | -----> LR = 0.0001
2020-03-22 14:02:25,909 | INFO | | Epoch: 007/100 | Train Time: 414.717 [s] | Train Loss: 0.644858 |
2020-03-22 14:02:25,909 | INFO | -----> LR = 0.0001
2020-03-22 14:09:20,904 | INFO | | Epoch: 008/100 | Train Time: 414.995 [s] | Train Loss: 0.625478 |
2020-03-22 14:09:20,904 | INFO | -----> LR = 0.0001
2020-03-22 14:16:15,689 | INFO | | Epoch: 009/100 | Train Time: 414.784 [s] | Train Loss: 0.625479 |
2020-03-22 14:16:15,689 | INFO | -----> LR = 0.0001
2020-03-22 14:23:10,669 | INFO | | Epoch: 010/100 | Train Time: 414.980 [s] | Train Loss: 0.592953 |
2020-03-22 14:23:10,669 | INFO | -----> LR = 0.0001
2020-03-22 14:30:05,364 | INFO | | Epoch: 011/100 | Train Time: 414.695 [s] | Train Loss: 0.602126 |
2020-03-22 14:30:05,365 | INFO | -----> LR = 0.0001
2020-03-22 14:37:00,102 | INFO | | Epoch: 012/100 | Train Time: 414.737 [s] | Train Loss: 0.574603 |
2020-03-22 14:37:00,103 | INFO | -----> LR = 0.0001
2020-03-22 14:43:55,097 | INFO | | Epoch: 013/100 | Train Time: 414.994 [s] | Train Loss: 0.566375 |
2020-03-22 14:43:55,098 | INFO | -----> LR = 0.0001
2020-03-22 14:50:49,979 | INFO | | Epoch: 014/100 | Train Time: 414.882 [s] | Train Loss: 0.552939 |
2020-03-22 14:50:49,980 | INFO | -----> LR = 0.0001
2020-03-22 14:57:44,786 | INFO | | Epoch: 015/100 | Train Time: 414.806 [s] | Train Loss: 0.545126 |
2020-03-22 14:57:44,786 | INFO | -----> LR = 0.0001
2020-03-22 15:04:39,721 | INFO | | Epoch: 016/100 | Train Time: 414.935 [s] | Train Loss: 0.545218 |
2020-03-22 15:04:39,721 | INFO | -----> LR = 0.0001
2020-03-22 15:11:34,624 | INFO | | Epoch: 017/100 | Train Time: 414.903 [s] | Train Loss: 0.525239 |
2020-03-22 15:11:34,625 | INFO | -----> LR = 0.0001
2020-03-22 15:18:29,602 | INFO | | Epoch: 018/100 | Train Time: 414.977 [s] | Train Loss: 0.521581 |
2020-03-22 15:18:29,602 | INFO | -----> LR = 0.0001
2020-03-22 15:25:24,505 | INFO | | Epoch: 019/100 | Train Time: 414.903 [s] | Train Loss: 0.525357 |
2020-03-22 15:25:24,505 | INFO | -----> LR = 0.0001
2020-03-22 15:32:19,562 | INFO | | Epoch: 020/100 | Train Time: 415.057 [s] | Train Loss: 0.517010 |
2020-03-22 15:32:19,563 | INFO | -----> LR = 0.0001
2020-03-22 15:39:14,426 | INFO | | Epoch: 021/100 | Train Time: 414.864 [s] | Train Loss: 0.509448 |
2020-03-22 15:39:14,427 | INFO | -----> LR = 0.0001
2020-03-22 15:46:09,463 | INFO | | Epoch: 022/100 | Train Time: 415.036 [s] | Train Loss: 0.500370 |
2020-03-22 15:46:09,463 | INFO | -----> LR = 0.0001
2020-03-22 15:53:04,599 | INFO | | Epoch: 023/100 | Train Time: 415.136 [s] | Train Loss: 0.504767 |
2020-03-22 15:53:04,600 | INFO | -----> LR = 0.0001
2020-03-22 15:59:59,635 | INFO | | Epoch: 024/100 | Train Time: 415.035 [s] | Train Loss: 0.489708 |
2020-03-22 15:59:59,635 | INFO | -----> LR = 0.0001
2020-03-22 16:06:54,616 | INFO | | Epoch: 025/100 | Train Time: 414.980 [s] | Train Loss: 0.483782 |
2020-03-22 16:06:54,616 | INFO | -----> LR = 0.0001
2020-03-22 16:13:49,754 | INFO | | Epoch: 026/100 | Train Time: 415.138 [s] | Train Loss: 0.483794 |
2020-03-22 16:13:49,755 | INFO | -----> LR = 0.0001
2020-03-22 16:20:44,680 | INFO | | Epoch: 027/100 | Train Time: 414.925 [s] | Train Loss: 0.479938 |
2020-03-22 16:20:44,681 | INFO | -----> LR = 0.0001
2020-03-22 16:27:39,800 | INFO | | Epoch: 028/100 | Train Time: 415.120 [s] | Train Loss: 0.478526 |
2020-03-22 16:27:39,801 | INFO | -----> LR = 0.0001
2020-03-22 16:34:34,593 | INFO | | Epoch: 029/100 | Train Time: 414.792 [s] | Train Loss: 0.472938 |
2020-03-22 16:34:34,593 | INFO | -----> LR = 0.0001
2020-03-22 16:41:29,689 | INFO | | Epoch: 030/100 | Train Time: 415.095 [s] | Train Loss: 0.474556 |
2020-03-22 16:41:29,689 | INFO | -----> LR = 0.0001
2020-03-22 16:48:24,793 | INFO | | Epoch: 031/100 | Train Time: 415.104 [s] | Train Loss: 0.467945 |
2020-03-22 16:48:24,793 | INFO | -----> LR = 0.0001
2020-03-22 16:55:19,868 | INFO | | Epoch: 032/100 | Train Time: 415.075 [s] | Train Loss: 0.464786 |
2020-03-22 16:55:19,869 | INFO | -----> LR = 0.0001
2020-03-22 17:02:14,844 | INFO | | Epoch: 033/100 | Train Time: 414.975 [s] | Train Loss: 0.464760 |
2020-03-22 17:02:14,844 | INFO | -----> LR = 0.0001
2020-03-22 17:09:09,936 | INFO | | Epoch: 034/100 | Train Time: 415.091 [s] | Train Loss: 0.460781 |
2020-03-22 17:09:09,936 | INFO | -----> LR = 0.0001
2020-03-22 17:16:04,728 | INFO | | Epoch: 035/100 | Train Time: 414.793 [s] | Train Loss: 0.459980 |
2020-03-22 17:16:04,729 | INFO | -----> LR = 0.0001
2020-03-22 17:22:59,588 | INFO | | Epoch: 036/100 | Train Time: 414.859 [s] | Train Loss: 0.456949 |
2020-03-22 17:22:59,588 | INFO | -----> LR = 0.0001
2020-03-22 17:29:54,414 | INFO | | Epoch: 037/100 | Train Time: 414.825 [s] | Train Loss: 0.461068 |
2020-03-22 17:29:54,414 | INFO | -----> LR = 0.0001
2020-03-22 17:36:48,962 | INFO | | Epoch: 038/100 | Train Time: 414.548 [s] | Train Loss: 0.451499 |
2020-03-22 17:36:48,963 | INFO | -----> LR = 0.0001
2020-03-22 17:43:43,664 | INFO | | Epoch: 039/100 | Train Time: 414.701 [s] | Train Loss: 0.449462 |
2020-03-22 17:43:43,665 | INFO | -----> LR = 0.0001
2020-03-22 17:50:38,587 | INFO | | Epoch: 040/100 | Train Time: 414.922 [s] | Train Loss: 0.451075 |
2020-03-22 17:50:38,588 | INFO | >>> LR Scheduler : new learning rate 1e-06
2020-03-22 17:50:38,588 | INFO | -----> LR = 1e-06
2020-03-22 17:57:33,451 | INFO | | Epoch: 041/100 | Train Time: 414.863 [s] | Train Loss: 0.420226 |
2020-03-22 17:57:33,452 | INFO | -----> LR = 1e-05
2020-03-22 18:04:28,228 | INFO | | Epoch: 042/100 | Train Time: 414.776 [s] | Train Loss: 0.416777 |
2020-03-22 18:04:28,229 | INFO | -----> LR = 1e-05
2020-03-22 18:11:22,841 | INFO | | Epoch: 043/100 | Train Time: 414.612 [s] | Train Loss: 0.414288 |
2020-03-22 18:11:22,842 | INFO | -----> LR = 1e-05
2020-03-22 18:18:17,398 | INFO | | Epoch: 044/100 | Train Time: 414.557 [s] | Train Loss: 0.408891 |
2020-03-22 18:18:17,399 | INFO | -----> LR = 1e-05
2020-03-22 18:25:12,106 | INFO | | Epoch: 045/100 | Train Time: 414.707 [s] | Train Loss: 0.410730 |
2020-03-22 18:25:12,107 | INFO | -----> LR = 1e-05
2020-03-22 18:32:06,627 | INFO | | Epoch: 046/100 | Train Time: 414.520 [s] | Train Loss: 0.409702 |
2020-03-22 18:32:06,627 | INFO | -----> LR = 1e-05
2020-03-22 18:39:01,263 | INFO | | Epoch: 047/100 | Train Time: 414.636 [s] | Train Loss: 0.402179 |
2020-03-22 18:39:01,264 | INFO | -----> LR = 1e-05
2020-03-22 18:45:55,959 | INFO | | Epoch: 048/100 | Train Time: 414.695 [s] | Train Loss: 0.408749 |
2020-03-22 18:45:55,960 | INFO | -----> LR = 1e-05
2020-03-22 18:52:50,916 | INFO | | Epoch: 049/100 | Train Time: 414.957 [s] | Train Loss: 0.404362 |
2020-03-22 18:52:50,917 | INFO | -----> LR = 1e-05
2020-03-22 18:59:45,791 | INFO | | Epoch: 050/100 | Train Time: 414.875 [s] | Train Loss: 0.402900 |
2020-03-22 18:59:45,792 | INFO | -----> LR = 1e-05
2020-03-22 19:06:40,511 | INFO | | Epoch: 051/100 | Train Time: 414.719 [s] | Train Loss: 0.405493 |
2020-03-22 19:06:40,512 | INFO | -----> LR = 1e-05
2020-03-22 19:13:35,310 | INFO | | Epoch: 052/100 | Train Time: 414.798 [s] | Train Loss: 0.405506 |
2020-03-22 19:13:35,311 | INFO | -----> LR = 1e-05
2020-03-22 19:20:30,335 | INFO | | Epoch: 053/100 | Train Time: 415.024 [s] | Train Loss: 0.401236 |
2020-03-22 19:20:30,335 | INFO | -----> LR = 1e-05
2020-03-22 19:27:25,114 | INFO | | Epoch: 054/100 | Train Time: 414.779 [s] | Train Loss: 0.405040 |
2020-03-22 19:27:25,115 | INFO | -----> LR = 1e-05
2020-03-22 19:34:20,107 | INFO | | Epoch: 055/100 | Train Time: 414.993 [s] | Train Loss: 0.401761 |
2020-03-22 19:34:20,108 | INFO | -----> LR = 1e-05
2020-03-22 19:41:15,009 | INFO | | Epoch: 056/100 | Train Time: 414.902 [s] | Train Loss: 0.402680 |
2020-03-22 19:41:15,010 | INFO | -----> LR = 1e-05
2020-03-22 19:48:10,183 | INFO | | Epoch: 057/100 | Train Time: 415.173 [s] | Train Loss: 0.402246 |
2020-03-22 19:48:10,184 | INFO | -----> LR = 1e-05
2020-03-22 19:55:05,326 | INFO | | Epoch: 058/100 | Train Time: 415.142 [s] | Train Loss: 0.400746 |
2020-03-22 19:55:05,327 | INFO | -----> LR = 1e-05
2020-03-22 20:02:00,262 | INFO | | Epoch: 059/100 | Train Time: 414.935 [s] | Train Loss: 0.398524 |
2020-03-22 20:02:00,262 | INFO | -----> LR = 1e-05
2020-03-22 20:08:55,121 | INFO | | Epoch: 060/100 | Train Time: 414.859 [s] | Train Loss: 0.400537 |
2020-03-22 20:08:55,121 | INFO | -----> LR = 1e-05
2020-03-22 20:15:50,277 | INFO | | Epoch: 061/100 | Train Time: 415.156 [s] | Train Loss: 0.397279 |
2020-03-22 20:15:50,278 | INFO | -----> LR = 1e-05
2020-03-22 20:22:45,474 | INFO | | Epoch: 062/100 | Train Time: 415.196 [s] | Train Loss: 0.401468 |
2020-03-22 20:22:45,475 | INFO | -----> LR = 1e-05
2020-03-22 20:29:40,532 | INFO | | Epoch: 063/100 | Train Time: 415.057 [s] | Train Loss: 0.398424 |
2020-03-22 20:29:40,533 | INFO | -----> LR = 1e-05
2020-03-22 20:36:35,713 | INFO | | Epoch: 064/100 | Train Time: 415.180 [s] | Train Loss: 0.395438 |
2020-03-22 20:36:35,714 | INFO | -----> LR = 1e-05
2020-03-22 20:43:30,626 | INFO | | Epoch: 065/100 | Train Time: 414.912 [s] | Train Loss: 0.394845 |
2020-03-22 20:43:30,627 | INFO | -----> LR = 1e-05
2020-03-22 20:50:25,605 | INFO | | Epoch: 066/100 | Train Time: 414.978 [s] | Train Loss: 0.395020 |
2020-03-22 20:50:25,605 | INFO | -----> LR = 1e-05
2020-03-22 20:57:20,502 | INFO | | Epoch: 067/100 | Train Time: 414.897 [s] | Train Loss: 0.395613 |
2020-03-22 20:57:20,503 | INFO | -----> LR = 1e-05
2020-03-22 21:04:15,407 | INFO | | Epoch: 068/100 | Train Time: 414.904 [s] | Train Loss: 0.393016 |
2020-03-22 21:04:15,407 | INFO | -----> LR = 1e-05
2020-03-22 21:11:10,216 | INFO | | Epoch: 069/100 | Train Time: 414.809 [s] | Train Loss: 0.391732 |
2020-03-22 21:11:10,216 | INFO | -----> LR = 1e-05
2020-03-22 21:18:05,069 | INFO | | Epoch: 070/100 | Train Time: 414.853 [s] | Train Loss: 0.395920 |
2020-03-22 21:18:05,070 | INFO | -----> LR = 1e-05
2020-03-22 21:24:59,921 | INFO | | Epoch: 071/100 | Train Time: 414.851 [s] | Train Loss: 0.393001 |
2020-03-22 21:24:59,921 | INFO | -----> LR = 1e-05
2020-03-22 21:31:54,824 | INFO | | Epoch: 072/100 | Train Time: 414.903 [s] | Train Loss: 0.394750 |
2020-03-22 21:31:54,824 | INFO | -----> LR = 1e-05
2020-03-22 21:38:49,535 | INFO | | Epoch: 073/100 | Train Time: 414.711 [s] | Train Loss: 0.391850 |
2020-03-22 21:38:49,536 | INFO | -----> LR = 1e-05
2020-03-22 21:45:44,186 | INFO | | Epoch: 074/100 | Train Time: 414.651 [s] | Train Loss: 0.394807 |
2020-03-22 21:45:44,187 | INFO | -----> LR = 1e-05
2020-03-22 21:52:39,016 | INFO | | Epoch: 075/100 | Train Time: 414.829 [s] | Train Loss: 0.396529 |
2020-03-22 21:52:39,016 | INFO | -----> LR = 1e-05
2020-03-22 21:59:33,622 | INFO | | Epoch: 076/100 | Train Time: 414.606 [s] | Train Loss: 0.391871 |
2020-03-22 21:59:33,623 | INFO | -----> LR = 1e-05
2020-03-22 22:06:28,506 | INFO | | Epoch: 077/100 | Train Time: 414.883 [s] | Train Loss: 0.395013 |
2020-03-22 22:06:28,506 | INFO | -----> LR = 1e-05
2020-03-22 22:13:23,280 | INFO | | Epoch: 078/100 | Train Time: 414.774 [s] | Train Loss: 0.392733 |
2020-03-22 22:13:23,281 | INFO | -----> LR = 1e-05
2020-03-22 22:20:18,050 | INFO | | Epoch: 079/100 | Train Time: 414.769 [s] | Train Loss: 0.391115 |
2020-03-22 22:20:18,051 | INFO | -----> LR = 1e-05
2020-03-22 22:27:12,639 | INFO | | Epoch: 080/100 | Train Time: 414.589 [s] | Train Loss: 0.392002 |
2020-03-22 22:27:12,640 | INFO | >>> LR Scheduler : new learning rate 1e-07
2020-03-22 22:27:12,640 | INFO | -----> LR = 1e-07
2020-03-22 22:34:07,412 | INFO | | Epoch: 081/100 | Train Time: 414.772 [s] | Train Loss: 0.387352 |
2020-03-22 22:34:07,413 | INFO | -----> LR = 1e-06
2020-03-22 22:41:01,973 | INFO | | Epoch: 082/100 | Train Time: 414.561 [s] | Train Loss: 0.387926 |
2020-03-22 22:41:01,973 | INFO | -----> LR = 1e-06
2020-03-22 22:47:56,385 | INFO | | Epoch: 083/100 | Train Time: 414.412 [s] | Train Loss: 0.390648 |
2020-03-22 22:47:56,386 | INFO | -----> LR = 1e-06
2020-03-22 22:54:51,044 | INFO | | Epoch: 084/100 | Train Time: 414.658 [s] | Train Loss: 0.385578 |
2020-03-22 22:54:51,045 | INFO | -----> LR = 1e-06
2020-03-22 23:01:45,628 | INFO | | Epoch: 085/100 | Train Time: 414.583 [s] | Train Loss: 0.390599 |
2020-03-22 23:01:45,628 | INFO | -----> LR = 1e-06
2020-03-22 23:08:39,841 | INFO | | Epoch: 086/100 | Train Time: 414.213 [s] | Train Loss: 0.385548 |
2020-03-22 23:08:39,841 | INFO | -----> LR = 1e-06
2020-03-22 23:15:34,300 | INFO | | Epoch: 087/100 | Train Time: 414.458 [s] | Train Loss: 0.393511 |
2020-03-22 23:15:34,300 | INFO | -----> LR = 1e-06
2020-03-22 23:22:28,552 | INFO | | Epoch: 088/100 | Train Time: 414.252 [s] | Train Loss: 0.384075 |
2020-03-22 23:22:28,553 | INFO | -----> LR = 1e-06
2020-03-22 23:29:22,940 | INFO | | Epoch: 089/100 | Train Time: 414.387 [s] | Train Loss: 0.383312 |
2020-03-22 23:29:22,941 | INFO | -----> LR = 1e-06
2020-03-22 23:36:17,267 | INFO | | Epoch: 090/100 | Train Time: 414.326 [s] | Train Loss: 0.384682 |
2020-03-22 23:36:17,268 | INFO | -----> LR = 1e-06
2020-03-22 23:43:11,576 | INFO | | Epoch: 091/100 | Train Time: 414.308 [s] | Train Loss: 0.386131 |
2020-03-22 23:43:11,577 | INFO | -----> LR = 1e-06
2020-03-22 23:50:05,975 | INFO | | Epoch: 092/100 | Train Time: 414.398 [s] | Train Loss: 0.387544 |
2020-03-22 23:50:05,976 | INFO | -----> LR = 1e-06
2020-03-22 23:57:00,287 | INFO | | Epoch: 093/100 | Train Time: 414.311 [s] | Train Loss: 0.386171 |
2020-03-22 23:57:00,288 | INFO | -----> LR = 1e-06
2020-03-23 00:03:54,755 | INFO | | Epoch: 094/100 | Train Time: 414.468 [s] | Train Loss: 0.386638 |
2020-03-23 00:03:54,756 | INFO | -----> LR = 1e-06
2020-03-23 00:10:49,047 | INFO | | Epoch: 095/100 | Train Time: 414.291 [s] | Train Loss: 0.387284 |
2020-03-23 00:10:49,047 | INFO | -----> LR = 1e-06
2020-03-23 00:17:43,423 | INFO | | Epoch: 096/100 | Train Time: 414.376 [s] | Train Loss: 0.388133 |
2020-03-23 00:17:43,424 | INFO | -----> LR = 1e-06
2020-03-23 00:24:37,887 | INFO | | Epoch: 097/100 | Train Time: 414.463 [s] | Train Loss: 0.390422 |
2020-03-23 00:24:37,887 | INFO | -----> LR = 1e-06
2020-03-23 00:31:32,313 | INFO | | Epoch: 098/100 | Train Time: 414.426 [s] | Train Loss: 0.386182 |
2020-03-23 00:31:32,314 | INFO | -----> LR = 1e-06
2020-03-23 00:38:26,553 | INFO | | Epoch: 099/100 | Train Time: 414.239 [s] | Train Loss: 0.385785 |
2020-03-23 00:38:26,554 | INFO | -----> LR = 1e-06
2020-03-23 00:45:20,630 | INFO | | Epoch: 100/100 | Train Time: 414.076 [s] | Train Loss: 0.382287 |
2020-03-23 00:45:20,630 | INFO | >>> Training of Joint DeepSAD and AutoEncoder Time: 41478.256 [s]
2020-03-23 00:45:20,631 | INFO | >>> Finished Joint DeepSAD and AutoEncoder Training.

2020-03-23 00:45:20,638 | INFO | >>> Start Validating of the joint DeepSAD and AutoEncoder.
2020-03-23 00:46:32,724 | INFO | >>> Validation Time: 71.741 [s]
2020-03-23 00:46:32,724 | INFO | >>> Validation Loss: 20.809348
2020-03-23 00:46:32,724 | INFO | >>> Validation reconstruction AUC: 53.931%
2020-03-23 00:46:32,724 | INFO | >>> Best Threshold for the reconstruction score maximizing F1-score: 0.000
2020-03-23 00:46:32,724 | INFO | >>> Best F1-score on reconstruction score: 84.558%
2020-03-23 00:46:32,724 | INFO | >>> Validation DeepSAD AUC: 74.645%
2020-03-23 00:46:32,724 | INFO | >>> Best Threshold for the DeepSAD score maximizing F1-score: 13.410
2020-03-23 00:46:32,724 | INFO | >>> Best F1-score on DeepSAD score: 84.627%
2020-03-23 00:46:32,724 | INFO | >>> Finished validating the Joint DeepSAD and AutoEncoder.

2020-03-23 00:46:32,731 | INFO | >>> Start Testing the joint DeepSAD and AutoEncoder.
2020-03-23 00:47:45,782 | INFO | >>> Test Time: 73.030 [s]
2020-03-23 00:47:45,782 | INFO | >>> Test Loss: 20.887086
2020-03-23 00:47:45,783 | INFO | >>> Test reconstruction AUC: 54.130%
2020-03-23 00:47:45,783 | INFO | >>> Test F1-score on reconstruction score: 84.850%
2020-03-23 00:47:45,783 | INFO | >>> Test AD AUC: 74.401%
2020-03-23 00:47:45,783 | INFO | >>> Test F1-score on DeepSAD score: 84.866%
2020-03-23 00:47:45,783 | INFO | >>> Finished Testing the Joint DeepSAD and AutoEncoder.

2020-03-23 00:47:46,098 | INFO | Test results saved at ../../../Outputs/JointDeepSAD_2020_03_22_00h21/results/JointDeepSAD_results_2.json

2020-03-23 00:47:46,181 | INFO | Model saved at ../../../Outputs/JointDeepSAD_2020_03_22_00h21/model/JointDeepSAD_model_2.pt

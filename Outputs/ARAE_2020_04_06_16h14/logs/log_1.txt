2020-04-06 16:14:02,286 | INFO | Brief summary of experiment : 

    Implementation of the unsupervised ARAE model proposed by Salehi et al (2020).
    This unsupervised method apply a projected gradient descent algorithm to find
    a more meaningful lattent space for the autoencoder. The encoder composed of
    a ResNet18 encoder followed by an average pooling layer. The decoder is
    composed of an upsampling layer followed by a mirrored ResNet18. The latent
    space has dimension (8,8,512).
    
2020-04-06 16:14:02,286 | INFO | Log file : ../../../Outputs/ARAE_2020_04_06_16h14/logs/log_1.txt
2020-04-06 16:14:02,286 | INFO | Data path : ../../../data/PROCESSED/
2020-04-06 16:14:02,286 | INFO | Outputs path : ../../../Outputs/ARAE_2020_04_06_16h14/

2020-04-06 16:14:06,047 | INFO | Train fraction : 50%
2020-04-06 16:14:06,047 | INFO | Fraction knonw normal : 0%
2020-04-06 16:14:06,047 | INFO | Fraction known abnormal : 0%
2020-04-06 16:14:06,077 | INFO | Split Summary 
+-------+----------------+------------+--------------+
| Set   | Name           | Number [-] | Fraction [%] |
+-------+----------------+------------+--------------+
| train | Normal         | 18975      | 100.00%      |
| train | Abnormal       | 0          | 0.00%        |
| train | Normal known   | 0          | 0.00%        |
| train | Abnormal known | 0          | 0.00%        |
| train | Unknown        | 18975      | 100.00%      |
| ----  | ----           | ----       | ----         |
| valid | Normal         | 2282       | 21.92%       |
| valid | Abnormal       | 8128       | 78.08%       |
| valid | Normal known   | 0          | 0.00%        |
| valid | Abnormal known | 0          | 0.00%        |
| valid | Unknown        | 10410      | 100.00%      |
| ----  | ----           | ----       | ----         |
| test  | Normal         | 2311       | 21.88%       |
| test  | Abnormal       | 8251       | 78.12%       |
| test  | Normal known   | 0          | 0.00%        |
| test  | Abnormal known | 0          | 0.00%        |
| test  | Unknown        | 10562      | 100.00%      |
+-------+----------------+------------+--------------+
2020-04-06 16:14:06,078 | INFO | Online preprocessing pipeline : 
             Grayscale()              
                  |                   
                  V                   
        AutoContrast(cutoff=1)        
                  |                   
                  V                   
     RandomHorizontalFlip(p=0.5)      
                  |                   
                  V                   
      RandomVerticalFlip(p=0.5)       
                  |                   
                  V                   
RandomBrightness(lower=0.8, upper=1.2)
                  |                   
                  V                   
RandomScaling(scale_range=(0.8, 1.2)) 
                  |                   
                  V                   
RandomRotation(degree_range=(-20, 20))
                  |                   
                  V                   
        ResizeMax(max_len=512)        
                  |                   
                  V                   
            PadToSquare()             
                  |                   
                  V                   
 MinMaxNormalization(vmin=0, vmax=1)  
                  |                   
                  V                   
           ToTorchTensor()            

2020-04-06 16:14:06,078 | INFO | Set seed 01/02 to 1
2020-04-06 16:14:06,078 | INFO | Device : cuda
2020-04-06 16:14:06,078 | INFO | Number of thread : 0
2020-04-06 16:14:06,078 | INFO | Number of dataloader worker for ARAE : 8

2020-04-06 16:14:08,470 | INFO | Network : AE_ResNet18
2020-04-06 16:14:08,471 | INFO | ResNet18 pretrained on ImageNet : False
2020-04-06 16:14:08,745 | INFO | Network architecture: 
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [16, 64, 256, 256]           9,408
       BatchNorm2d-2         [16, 64, 256, 256]               0
              ReLU-3         [16, 64, 256, 256]               0
         MaxPool2d-4         [16, 64, 128, 128]               0
            Conv2d-5         [16, 64, 128, 128]          36,864
       BatchNorm2d-6         [16, 64, 128, 128]               0
              ReLU-7         [16, 64, 128, 128]               0
            Conv2d-8         [16, 64, 128, 128]          36,864
       BatchNorm2d-9         [16, 64, 128, 128]               0
             ReLU-10         [16, 64, 128, 128]               0
     DownResBlock-11         [16, 64, 128, 128]               0
           Conv2d-12         [16, 64, 128, 128]          36,864
      BatchNorm2d-13         [16, 64, 128, 128]               0
             ReLU-14         [16, 64, 128, 128]               0
           Conv2d-15         [16, 64, 128, 128]          36,864
      BatchNorm2d-16         [16, 64, 128, 128]               0
             ReLU-17         [16, 64, 128, 128]               0
     DownResBlock-18         [16, 64, 128, 128]               0
           Conv2d-19          [16, 128, 64, 64]           8,192
      BatchNorm2d-20          [16, 128, 64, 64]               0
           Conv2d-21          [16, 128, 64, 64]          73,728
      BatchNorm2d-22          [16, 128, 64, 64]               0
             ReLU-23          [16, 128, 64, 64]               0
           Conv2d-24          [16, 128, 64, 64]         147,456
      BatchNorm2d-25          [16, 128, 64, 64]               0
             ReLU-26          [16, 128, 64, 64]               0
     DownResBlock-27          [16, 128, 64, 64]               0
           Conv2d-28          [16, 128, 64, 64]         147,456
      BatchNorm2d-29          [16, 128, 64, 64]               0
             ReLU-30          [16, 128, 64, 64]               0
           Conv2d-31          [16, 128, 64, 64]         147,456
      BatchNorm2d-32          [16, 128, 64, 64]               0
             ReLU-33          [16, 128, 64, 64]               0
     DownResBlock-34          [16, 128, 64, 64]               0
           Conv2d-35          [16, 256, 32, 32]          32,768
      BatchNorm2d-36          [16, 256, 32, 32]               0
           Conv2d-37          [16, 256, 32, 32]         294,912
      BatchNorm2d-38          [16, 256, 32, 32]               0
             ReLU-39          [16, 256, 32, 32]               0
           Conv2d-40          [16, 256, 32, 32]         589,824
      BatchNorm2d-41          [16, 256, 32, 32]               0
             ReLU-42          [16, 256, 32, 32]               0
     DownResBlock-43          [16, 256, 32, 32]               0
           Conv2d-44          [16, 256, 32, 32]         589,824
      BatchNorm2d-45          [16, 256, 32, 32]               0
             ReLU-46          [16, 256, 32, 32]               0
           Conv2d-47          [16, 256, 32, 32]         589,824
      BatchNorm2d-48          [16, 256, 32, 32]               0
             ReLU-49          [16, 256, 32, 32]               0
     DownResBlock-50          [16, 256, 32, 32]               0
           Conv2d-51          [16, 512, 16, 16]         131,072
      BatchNorm2d-52          [16, 512, 16, 16]               0
           Conv2d-53          [16, 512, 16, 16]       1,179,648
      BatchNorm2d-54          [16, 512, 16, 16]               0
             ReLU-55          [16, 512, 16, 16]               0
           Conv2d-56          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-57          [16, 512, 16, 16]               0
             ReLU-58          [16, 512, 16, 16]               0
     DownResBlock-59          [16, 512, 16, 16]               0
           Conv2d-60          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-61          [16, 512, 16, 16]               0
             ReLU-62          [16, 512, 16, 16]               0
           Conv2d-63          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-64          [16, 512, 16, 16]               0
             ReLU-65          [16, 512, 16, 16]               0
     DownResBlock-66          [16, 512, 16, 16]               0
 ResNet18_Encoder-67          [16, 512, 16, 16]               0
        AvgPool2d-68            [16, 512, 8, 8]               0
         Upsample-69          [16, 512, 16, 16]               0
           Conv2d-70          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-71          [16, 512, 16, 16]               0
             ReLU-72          [16, 512, 16, 16]               0
           Conv2d-73          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-74          [16, 512, 16, 16]               0
             ReLU-75          [16, 512, 16, 16]               0
       UpResBlock-76          [16, 512, 16, 16]               0
           Conv2d-77          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-78          [16, 512, 16, 16]               0
             ReLU-79          [16, 512, 16, 16]               0
         Upsample-80          [16, 512, 32, 32]               0
           Conv2d-81          [16, 256, 32, 32]         131,072
      BatchNorm2d-82          [16, 256, 32, 32]               0
         Upsample-83          [16, 512, 32, 32]               0
           Conv2d-84          [16, 256, 32, 32]         131,072
      BatchNorm2d-85          [16, 256, 32, 32]               0
             ReLU-86          [16, 256, 32, 32]               0
       UpResBlock-87          [16, 256, 32, 32]               0
           Conv2d-88          [16, 256, 32, 32]         589,824
      BatchNorm2d-89          [16, 256, 32, 32]               0
             ReLU-90          [16, 256, 32, 32]               0
           Conv2d-91          [16, 256, 32, 32]         589,824
      BatchNorm2d-92          [16, 256, 32, 32]               0
             ReLU-93          [16, 256, 32, 32]               0
       UpResBlock-94          [16, 256, 32, 32]               0
           Conv2d-95          [16, 256, 32, 32]         589,824
      BatchNorm2d-96          [16, 256, 32, 32]               0
             ReLU-97          [16, 256, 32, 32]               0
         Upsample-98          [16, 256, 64, 64]               0
           Conv2d-99          [16, 128, 64, 64]          32,768
     BatchNorm2d-100          [16, 128, 64, 64]               0
        Upsample-101          [16, 256, 64, 64]               0
          Conv2d-102          [16, 128, 64, 64]          32,768
     BatchNorm2d-103          [16, 128, 64, 64]               0
            ReLU-104          [16, 128, 64, 64]               0
      UpResBlock-105          [16, 128, 64, 64]               0
          Conv2d-106          [16, 128, 64, 64]         147,456
     BatchNorm2d-107          [16, 128, 64, 64]               0
            ReLU-108          [16, 128, 64, 64]               0
          Conv2d-109          [16, 128, 64, 64]         147,456
     BatchNorm2d-110          [16, 128, 64, 64]               0
            ReLU-111          [16, 128, 64, 64]               0
      UpResBlock-112          [16, 128, 64, 64]               0
          Conv2d-113          [16, 128, 64, 64]         147,456
     BatchNorm2d-114          [16, 128, 64, 64]               0
            ReLU-115          [16, 128, 64, 64]               0
        Upsample-116        [16, 128, 128, 128]               0
          Conv2d-117         [16, 64, 128, 128]           8,192
     BatchNorm2d-118         [16, 64, 128, 128]               0
        Upsample-119        [16, 128, 128, 128]               0
          Conv2d-120         [16, 64, 128, 128]           8,192
     BatchNorm2d-121         [16, 64, 128, 128]               0
            ReLU-122         [16, 64, 128, 128]               0
      UpResBlock-123         [16, 64, 128, 128]               0
          Conv2d-124         [16, 64, 128, 128]          36,864
     BatchNorm2d-125         [16, 64, 128, 128]               0
            ReLU-126         [16, 64, 128, 128]               0
          Conv2d-127         [16, 64, 128, 128]          36,864
     BatchNorm2d-128         [16, 64, 128, 128]               0
            ReLU-129         [16, 64, 128, 128]               0
      UpResBlock-130         [16, 64, 128, 128]               0
          Conv2d-131         [16, 64, 128, 128]          36,864
     BatchNorm2d-132         [16, 64, 128, 128]               0
            ReLU-133         [16, 64, 128, 128]               0
        Upsample-134         [16, 64, 256, 256]               0
          Conv2d-135         [16, 64, 256, 256]           4,096
     BatchNorm2d-136         [16, 64, 256, 256]               0
        Upsample-137         [16, 64, 256, 256]               0
          Conv2d-138         [16, 64, 256, 256]           4,096
     BatchNorm2d-139         [16, 64, 256, 256]               0
            ReLU-140         [16, 64, 256, 256]               0
      UpResBlock-141         [16, 64, 256, 256]               0
        Upsample-142         [16, 64, 512, 512]               0
          Conv2d-143          [16, 1, 512, 512]              64
            Tanh-144          [16, 1, 512, 512]               0
ResNet18_Decoder-145          [16, 1, 512, 512]               0
================================================================
Total params: 20,919,552
Trainable params: 20,919,552
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 16.00
Forward/backward pass size (MB): 16164.00
Params size (MB): 79.80
Estimated Total Size (MB): 16259.80
----------------------------------------------------------------


2020-04-06 16:14:08,745 | INFO | ARAE epsilon : 0.05
2020-04-06 16:14:08,745 | INFO | ARAE adversarial importance gamma : 0.1
2020-04-06 16:14:08,745 | INFO | ARAE number of epoch : 50
2020-04-06 16:14:08,745 | INFO | ARAE number of adversarial search epoch: 4
2020-04-06 16:14:08,745 | INFO | ARAE learning rate : 0.001
2020-04-06 16:14:08,745 | INFO | ARAE adversarial search learning rate : 0.1
2020-04-06 16:14:08,745 | INFO | ARAE learning rate milestone : [25, 40]
2020-04-06 16:14:08,745 | INFO | ARAE weight_decay : 1e-06
2020-04-06 16:14:08,745 | INFO | ARAE optimizer : Adam
2020-04-06 16:14:08,745 | INFO | ARAE batch_size 16
2020-04-06 16:14:08,746 | INFO | ARAE number of dataloader worker : 8
2020-04-06 16:14:08,748 | INFO | >>> Start Training the ARAE.
2020-04-06 16:44:31,746 | INFO | | Epoch: 001/050 | Train Time: 1822.998 [s] | Train Loss: 0.018636 |
2020-04-06 17:15:00,577 | INFO | | Epoch: 002/050 | Train Time: 1828.830 [s] | Train Loss: 0.013053 |
2020-04-06 17:45:31,877 | INFO | | Epoch: 003/050 | Train Time: 1831.300 [s] | Train Loss: 0.010974 |
2020-04-06 18:16:03,628 | INFO | | Epoch: 004/050 | Train Time: 1831.750 [s] | Train Loss: 0.009498 |
2020-04-06 18:46:35,456 | INFO | | Epoch: 005/050 | Train Time: 1831.828 [s] | Train Loss: 0.008729 |
2020-04-06 19:17:08,436 | INFO | | Epoch: 006/050 | Train Time: 1832.979 [s] | Train Loss: 0.008392 |
2020-04-06 19:47:41,535 | INFO | | Epoch: 007/050 | Train Time: 1833.098 [s] | Train Loss: 0.008459 |
2020-04-06 20:18:14,043 | INFO | | Epoch: 008/050 | Train Time: 1832.508 [s] | Train Loss: 0.008238 |
2020-04-06 20:48:46,714 | INFO | | Epoch: 009/050 | Train Time: 1832.671 [s] | Train Loss: 0.008210 |
2020-04-06 21:19:18,248 | INFO | | Epoch: 010/050 | Train Time: 1831.534 [s] | Train Loss: 0.008254 |
2020-04-06 21:49:49,841 | INFO | | Epoch: 011/050 | Train Time: 1831.593 [s] | Train Loss: 0.008161 |
2020-04-06 22:20:22,599 | INFO | | Epoch: 012/050 | Train Time: 1832.758 [s] | Train Loss: 0.007908 |
2020-04-06 22:50:53,391 | INFO | | Epoch: 013/050 | Train Time: 1830.792 [s] | Train Loss: 0.007584 |
2020-04-06 23:21:23,617 | INFO | | Epoch: 014/050 | Train Time: 1830.225 [s] | Train Loss: 0.007453 |
2020-04-06 23:51:51,006 | INFO | | Epoch: 015/050 | Train Time: 1827.389 [s] | Train Loss: 0.007304 |
2020-04-07 00:22:18,294 | INFO | | Epoch: 016/050 | Train Time: 1827.288 [s] | Train Loss: 0.007146 |
2020-04-07 00:52:44,446 | INFO | | Epoch: 017/050 | Train Time: 1826.151 [s] | Train Loss: 0.007040 |
2020-04-07 01:23:08,576 | INFO | | Epoch: 018/050 | Train Time: 1824.129 [s] | Train Loss: 0.009160 |
2020-04-07 01:53:32,246 | INFO | | Epoch: 019/050 | Train Time: 1823.670 [s] | Train Loss: 0.010095 |
2020-04-07 02:23:56,507 | INFO | | Epoch: 020/050 | Train Time: 1824.260 [s] | Train Loss: 0.007902 |
2020-04-07 02:54:21,247 | INFO | | Epoch: 021/050 | Train Time: 1824.739 [s] | Train Loss: 0.007223 |
2020-04-07 03:24:45,733 | INFO | | Epoch: 022/050 | Train Time: 1824.486 [s] | Train Loss: 0.007010 |
2020-04-07 03:55:10,130 | INFO | | Epoch: 023/050 | Train Time: 1824.397 [s] | Train Loss: 0.006993 |
2020-04-07 04:25:34,088 | INFO | | Epoch: 024/050 | Train Time: 1823.958 [s] | Train Loss: 0.006817 |
2020-04-07 04:55:56,614 | INFO | | Epoch: 025/050 | Train Time: 1822.525 [s] | Train Loss: 0.006719 |
2020-04-07 04:55:56,614 | INFO | >>> LR Scheduler : new learning rate 1e-05
2020-04-07 05:26:19,306 | INFO | | Epoch: 026/050 | Train Time: 1822.691 [s] | Train Loss: 0.005480 |
2020-04-07 05:56:42,394 | INFO | | Epoch: 027/050 | Train Time: 1823.087 [s] | Train Loss: 0.005147 |
2020-04-07 06:27:04,946 | INFO | | Epoch: 028/050 | Train Time: 1822.551 [s] | Train Loss: 0.005013 |
2020-04-07 06:57:27,621 | INFO | | Epoch: 029/050 | Train Time: 1822.675 [s] | Train Loss: 0.004838 |
2020-04-07 07:27:49,312 | INFO | | Epoch: 030/050 | Train Time: 1821.691 [s] | Train Loss: 0.004705 |
2020-04-07 07:58:09,848 | INFO | | Epoch: 031/050 | Train Time: 1820.536 [s] | Train Loss: 0.004613 |
2020-04-07 08:28:30,958 | INFO | | Epoch: 032/050 | Train Time: 1821.109 [s] | Train Loss: 0.004549 |
2020-04-07 08:58:52,369 | INFO | | Epoch: 033/050 | Train Time: 1821.411 [s] | Train Loss: 0.004413 |
2020-04-07 09:29:15,023 | INFO | | Epoch: 034/050 | Train Time: 1822.653 [s] | Train Loss: 0.004319 |
2020-04-07 09:59:38,853 | INFO | | Epoch: 035/050 | Train Time: 1823.829 [s] | Train Loss: 0.004264 |
2020-04-07 10:30:03,649 | INFO | | Epoch: 036/050 | Train Time: 1824.795 [s] | Train Loss: 0.004156 |
2020-04-07 11:00:27,493 | INFO | | Epoch: 037/050 | Train Time: 1823.844 [s] | Train Loss: 0.004056 |
2020-04-07 11:30:52,037 | INFO | | Epoch: 038/050 | Train Time: 1824.544 [s] | Train Loss: 0.004001 |
2020-04-07 12:01:16,982 | INFO | | Epoch: 039/050 | Train Time: 1824.944 [s] | Train Loss: 0.003929 |
2020-04-07 12:31:42,340 | INFO | | Epoch: 040/050 | Train Time: 1825.357 [s] | Train Loss: 0.003850 |
2020-04-07 12:31:42,341 | INFO | >>> LR Scheduler : new learning rate 1e-06
2020-04-07 13:02:07,903 | INFO | | Epoch: 041/050 | Train Time: 1825.562 [s] | Train Loss: 0.003529 |
2020-04-07 13:32:33,753 | INFO | | Epoch: 042/050 | Train Time: 1825.849 [s] | Train Loss: 0.003477 |
2020-04-07 14:03:00,817 | INFO | | Epoch: 043/050 | Train Time: 1827.064 [s] | Train Loss: 0.003441 |
2020-04-07 14:33:28,946 | INFO | | Epoch: 044/050 | Train Time: 1828.128 [s] | Train Loss: 0.003398 |
2020-04-07 15:03:57,614 | INFO | | Epoch: 045/050 | Train Time: 1828.668 [s] | Train Loss: 0.003392 |
2020-04-07 15:34:26,951 | INFO | | Epoch: 046/050 | Train Time: 1829.336 [s] | Train Loss: 0.003360 |
2020-04-07 16:04:56,361 | INFO | | Epoch: 047/050 | Train Time: 1829.409 [s] | Train Loss: 0.003350 |
2020-04-07 16:35:26,593 | INFO | | Epoch: 048/050 | Train Time: 1830.232 [s] | Train Loss: 0.003307 |
2020-04-07 17:05:58,686 | INFO | | Epoch: 049/050 | Train Time: 1832.093 [s] | Train Loss: 0.003297 |
2020-04-07 17:36:32,555 | INFO | | Epoch: 050/050 | Train Time: 1833.869 [s] | Train Loss: 0.003269 |
2020-04-07 17:36:32,556 | INFO | >>> Training Time of ARAE: 91343.808 [s]
2020-04-07 17:36:32,556 | INFO | >>> Finished ARAE Training.

2020-04-07 17:36:32,563 | INFO | >>> Start Validating of the ARAE.
2020-04-07 17:37:44,089 | INFO | >>> Validation Time: 71.295 [s]
2020-04-07 17:37:44,089 | INFO | >>> Validation Loss: 0.003929
2020-04-07 17:37:44,089 | INFO | >>> Validation AUC: 51.037%
2020-04-07 17:37:44,089 | INFO | >>> Best Threshold for the score maximizing F1-score: 0.000
2020-04-07 17:37:44,089 | INFO | >>> Best F1-score: 87.675%
2020-04-07 17:37:44,089 | INFO | >>> Finished validating the ARAE.

2020-04-07 17:37:44,095 | INFO | >>> Start Testing of the ARAE.
2020-04-07 17:38:57,331 | INFO | >>> Testing Time: 73.224 [s]
2020-04-07 17:38:57,332 | INFO | >>> Test Loss: 0.003927
2020-04-07 17:38:57,332 | INFO | >>> Test AUC: 52.391%
2020-04-07 17:38:57,332 | INFO | >>> Test F1-score: 87.673%
2020-04-07 17:38:57,332 | INFO | >>> Finished testing the ARAE.

2020-04-07 17:38:57,483 | INFO | Test results saved at ../../../Outputs/ARAE_2020_04_06_16h14/results/ARAE_results_1.json

2020-04-07 17:38:57,558 | INFO | Model saved at ../../../Outputs/ARAE_2020_04_06_16h14/model/ARAE_model_1.pt

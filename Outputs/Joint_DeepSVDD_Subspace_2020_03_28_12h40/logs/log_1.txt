2020-03-28 12:40:45,369 | INFO | Brief summary of experiment : 

    Train jointly the AutoEncoder and the DeepSAD model following Lukas Ruff et
    al. (2019) work adapted to the MURA dataset (preprocessing inspired from the
    work of Davletshina et al. (2020)). The network structure is a ResNet18
    AutoEncoder until the Adaptative average pooling layer. The AE embdedding is
    thus (512, 16, 16). This embdedding is further processed through 3 convolutional
    layers (specific to the SVDD embdedding generation) to provide the SVDD
    embdedding of 512. The network is trained with two loss functions: a masked MSE
    loss for the reconstruction and the DeepSAD loss on the embedding. The two
    losses are scaled to be comparable by perfoming one forward pass prior the
    training. The Encoder is not initialized with weights trained on ImageNet.
    The AE masked reconstruction loss is not computed for known abnormal sample
    so that the AE learn to reconstruct normal samples only. The network input is
    masked with the mask : only the body part is kept and the background is set
    to zero. The AE is pretrained over 5 epochs in order to improve the initialization
    of the hypersphere center (we hypothetize that with a pretrained AE the
    hypersphere center estimation will be more meaningful). Note that the 'affine'
    parameters of BatchNorm2d layers has been changed to False in this implementation.
    The SVDD anomaly score used is the projection on the subspace of normal training
    sample inpired from the few-sho-classification framework in Devos et al. (2019).
    
2020-03-28 12:40:45,370 | INFO | Log file : ../../../Outputs/Joint_DeepSVDD_Subspace_2020_03_28_12h40/logs/log_1.txt
2020-03-28 12:40:45,370 | INFO | Data path : ../../../data/PROCESSED/
2020-03-28 12:40:45,370 | INFO | Outputs path : ../../../Outputs/Joint_DeepSVDD_Subspace_2020_03_28_12h40/

2020-03-28 12:40:49,097 | INFO | Train fraction : 50%
2020-03-28 12:40:49,097 | INFO | Fraction knonw normal : 0%
2020-03-28 12:40:49,098 | INFO | Fraction known abnormal : 0%
2020-03-28 12:40:49,127 | INFO | Split Summary 
+-------+----------------+------------+--------------+
| Set   | Name           | Number [-] | Fraction [%] |
+-------+----------------+------------+--------------+
| train | Normal         | 18975      | 100.00%      |
| train | Abnormal       | 0          | 0.00%        |
| train | Normal known   | 0          | 0.00%        |
| train | Abnormal known | 0          | 0.00%        |
| train | Unknown        | 18975      | 100.00%      |
| ----  | ----           | ----       | ----         |
| valid | Normal         | 2282       | 21.92%       |
| valid | Abnormal       | 8128       | 78.08%       |
| valid | Normal known   | 0          | 0.00%        |
| valid | Abnormal known | 0          | 0.00%        |
| valid | Unknown        | 10410      | 100.00%      |
| ----  | ----           | ----       | ----         |
| test  | Normal         | 2311       | 21.88%       |
| test  | Abnormal       | 8251       | 78.12%       |
| test  | Normal known   | 0          | 0.00%        |
| test  | Abnormal known | 0          | 0.00%        |
| test  | Unknown        | 10562      | 100.00%      |
+-------+----------------+------------+--------------+
2020-03-28 12:40:49,128 | INFO | Online preprocessing pipeline : 
             Grayscale()              
                  |                   
                  V                   
        AutoContrast(cutoff=1)        
                  |                   
                  V                   
     RandomHorizontalFlip(p=0.5)      
                  |                   
                  V                   
      RandomVerticalFlip(p=0.5)       
                  |                   
                  V                   
RandomBrightness(lower=0.8, upper=1.2)
                  |                   
                  V                   
RandomScaling(scale_range=(0.8, 1.2)) 
                  |                   
                  V                   
RandomRotation(degree_range=(-20, 20))
                  |                   
                  V                   
        ResizeMax(max_len=512)        
                  |                   
                  V                   
            PadToSquare()             
                  |                   
                  V                   
 MinMaxNormalization(vmin=0, vmax=1)  
                  |                   
                  V                   
           ToTorchTensor()            

2020-03-28 12:40:49,128 | INFO | Set seed 01/04 to 1
2020-03-28 12:40:49,128 | INFO | Device : cuda
2020-03-28 12:40:49,128 | INFO | Number of thread : 0
2020-03-28 12:40:49,128 | INFO | Number of dataloader worker for Joint_DeepSVDD_Subspace : 8

2020-03-28 12:40:51,516 | INFO | Network : AE_SVDD_Hybrid
2020-03-28 12:40:51,516 | INFO | Autoencoder pretrained on ImageNet : False
2020-03-28 12:40:51,516 | INFO | Joint_DeepSVDD_Subspace eta : 0.0
2020-03-28 12:40:51,789 | INFO | Network architecture: 
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [16, 64, 256, 256]           9,408
       BatchNorm2d-2         [16, 64, 256, 256]               0
              ReLU-3         [16, 64, 256, 256]               0
         MaxPool2d-4         [16, 64, 128, 128]               0
            Conv2d-5         [16, 64, 128, 128]          36,864
       BatchNorm2d-6         [16, 64, 128, 128]               0
              ReLU-7         [16, 64, 128, 128]               0
            Conv2d-8         [16, 64, 128, 128]          36,864
       BatchNorm2d-9         [16, 64, 128, 128]               0
             ReLU-10         [16, 64, 128, 128]               0
     DownResBlock-11         [16, 64, 128, 128]               0
           Conv2d-12         [16, 64, 128, 128]          36,864
      BatchNorm2d-13         [16, 64, 128, 128]               0
             ReLU-14         [16, 64, 128, 128]               0
           Conv2d-15         [16, 64, 128, 128]          36,864
      BatchNorm2d-16         [16, 64, 128, 128]               0
             ReLU-17         [16, 64, 128, 128]               0
     DownResBlock-18         [16, 64, 128, 128]               0
           Conv2d-19          [16, 128, 64, 64]           8,192
      BatchNorm2d-20          [16, 128, 64, 64]               0
           Conv2d-21          [16, 128, 64, 64]          73,728
      BatchNorm2d-22          [16, 128, 64, 64]               0
             ReLU-23          [16, 128, 64, 64]               0
           Conv2d-24          [16, 128, 64, 64]         147,456
      BatchNorm2d-25          [16, 128, 64, 64]               0
             ReLU-26          [16, 128, 64, 64]               0
     DownResBlock-27          [16, 128, 64, 64]               0
           Conv2d-28          [16, 128, 64, 64]         147,456
      BatchNorm2d-29          [16, 128, 64, 64]               0
             ReLU-30          [16, 128, 64, 64]               0
           Conv2d-31          [16, 128, 64, 64]         147,456
      BatchNorm2d-32          [16, 128, 64, 64]               0
             ReLU-33          [16, 128, 64, 64]               0
     DownResBlock-34          [16, 128, 64, 64]               0
           Conv2d-35          [16, 256, 32, 32]          32,768
      BatchNorm2d-36          [16, 256, 32, 32]               0
           Conv2d-37          [16, 256, 32, 32]         294,912
      BatchNorm2d-38          [16, 256, 32, 32]               0
             ReLU-39          [16, 256, 32, 32]               0
           Conv2d-40          [16, 256, 32, 32]         589,824
      BatchNorm2d-41          [16, 256, 32, 32]               0
             ReLU-42          [16, 256, 32, 32]               0
     DownResBlock-43          [16, 256, 32, 32]               0
           Conv2d-44          [16, 256, 32, 32]         589,824
      BatchNorm2d-45          [16, 256, 32, 32]               0
             ReLU-46          [16, 256, 32, 32]               0
           Conv2d-47          [16, 256, 32, 32]         589,824
      BatchNorm2d-48          [16, 256, 32, 32]               0
             ReLU-49          [16, 256, 32, 32]               0
     DownResBlock-50          [16, 256, 32, 32]               0
           Conv2d-51          [16, 512, 16, 16]         131,072
      BatchNorm2d-52          [16, 512, 16, 16]               0
           Conv2d-53          [16, 512, 16, 16]       1,179,648
      BatchNorm2d-54          [16, 512, 16, 16]               0
             ReLU-55          [16, 512, 16, 16]               0
           Conv2d-56          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-57          [16, 512, 16, 16]               0
             ReLU-58          [16, 512, 16, 16]               0
     DownResBlock-59          [16, 512, 16, 16]               0
           Conv2d-60          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-61          [16, 512, 16, 16]               0
             ReLU-62          [16, 512, 16, 16]               0
           Conv2d-63          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-64          [16, 512, 16, 16]               0
             ReLU-65          [16, 512, 16, 16]               0
     DownResBlock-66          [16, 512, 16, 16]               0
 ResNet18_Encoder-67          [16, 512, 16, 16]               0
           Conv2d-68          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-69          [16, 512, 16, 16]               0
             ReLU-70          [16, 512, 16, 16]               0
           Conv2d-71          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-72          [16, 512, 16, 16]               0
             ReLU-73          [16, 512, 16, 16]               0
       UpResBlock-74          [16, 512, 16, 16]               0
           Conv2d-75          [16, 512, 16, 16]       2,359,296
      BatchNorm2d-76          [16, 512, 16, 16]               0
             ReLU-77          [16, 512, 16, 16]               0
         Upsample-78          [16, 512, 32, 32]               0
           Conv2d-79          [16, 256, 32, 32]         131,072
      BatchNorm2d-80          [16, 256, 32, 32]               0
         Upsample-81          [16, 512, 32, 32]               0
           Conv2d-82          [16, 256, 32, 32]         131,072
      BatchNorm2d-83          [16, 256, 32, 32]               0
             ReLU-84          [16, 256, 32, 32]               0
       UpResBlock-85          [16, 256, 32, 32]               0
           Conv2d-86          [16, 256, 32, 32]         589,824
      BatchNorm2d-87          [16, 256, 32, 32]               0
             ReLU-88          [16, 256, 32, 32]               0
           Conv2d-89          [16, 256, 32, 32]         589,824
      BatchNorm2d-90          [16, 256, 32, 32]               0
             ReLU-91          [16, 256, 32, 32]               0
       UpResBlock-92          [16, 256, 32, 32]               0
           Conv2d-93          [16, 256, 32, 32]         589,824
      BatchNorm2d-94          [16, 256, 32, 32]               0
             ReLU-95          [16, 256, 32, 32]               0
         Upsample-96          [16, 256, 64, 64]               0
           Conv2d-97          [16, 128, 64, 64]          32,768
      BatchNorm2d-98          [16, 128, 64, 64]               0
         Upsample-99          [16, 256, 64, 64]               0
          Conv2d-100          [16, 128, 64, 64]          32,768
     BatchNorm2d-101          [16, 128, 64, 64]               0
            ReLU-102          [16, 128, 64, 64]               0
      UpResBlock-103          [16, 128, 64, 64]               0
          Conv2d-104          [16, 128, 64, 64]         147,456
     BatchNorm2d-105          [16, 128, 64, 64]               0
            ReLU-106          [16, 128, 64, 64]               0
          Conv2d-107          [16, 128, 64, 64]         147,456
     BatchNorm2d-108          [16, 128, 64, 64]               0
            ReLU-109          [16, 128, 64, 64]               0
      UpResBlock-110          [16, 128, 64, 64]               0
          Conv2d-111          [16, 128, 64, 64]         147,456
     BatchNorm2d-112          [16, 128, 64, 64]               0
            ReLU-113          [16, 128, 64, 64]               0
        Upsample-114        [16, 128, 128, 128]               0
          Conv2d-115         [16, 64, 128, 128]           8,192
     BatchNorm2d-116         [16, 64, 128, 128]               0
        Upsample-117        [16, 128, 128, 128]               0
          Conv2d-118         [16, 64, 128, 128]           8,192
     BatchNorm2d-119         [16, 64, 128, 128]               0
            ReLU-120         [16, 64, 128, 128]               0
      UpResBlock-121         [16, 64, 128, 128]               0
          Conv2d-122         [16, 64, 128, 128]          36,864
     BatchNorm2d-123         [16, 64, 128, 128]               0
            ReLU-124         [16, 64, 128, 128]               0
          Conv2d-125         [16, 64, 128, 128]          36,864
     BatchNorm2d-126         [16, 64, 128, 128]               0
            ReLU-127         [16, 64, 128, 128]               0
      UpResBlock-128         [16, 64, 128, 128]               0
          Conv2d-129         [16, 64, 128, 128]          36,864
     BatchNorm2d-130         [16, 64, 128, 128]               0
            ReLU-131         [16, 64, 128, 128]               0
        Upsample-132         [16, 64, 256, 256]               0
          Conv2d-133         [16, 64, 256, 256]           4,096
     BatchNorm2d-134         [16, 64, 256, 256]               0
        Upsample-135         [16, 64, 256, 256]               0
          Conv2d-136         [16, 64, 256, 256]           4,096
     BatchNorm2d-137         [16, 64, 256, 256]               0
            ReLU-138         [16, 64, 256, 256]               0
      UpResBlock-139         [16, 64, 256, 256]               0
        Upsample-140         [16, 64, 512, 512]               0
          Conv2d-141          [16, 1, 512, 512]              64
            Tanh-142          [16, 1, 512, 512]               0
ResNet18_Decoder-143          [16, 1, 512, 512]               0
       AvgPool2d-144            [16, 512, 7, 7]               0
          Conv2d-145            [16, 256, 5, 5]       1,179,648
     BatchNorm2d-146            [16, 256, 5, 5]               0
            ReLU-147            [16, 256, 5, 5]               0
       MaxPool2d-148            [16, 256, 2, 2]               0
          Conv2d-149            [16, 128, 2, 2]          32,768
     BatchNorm2d-150            [16, 128, 2, 2]               0
================================================================
Total params: 22,131,968
Trainable params: 22,131,968
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 16.00
Forward/backward pass size (MB): 16149.66
Params size (MB): 84.43
Estimated Total Size (MB): 16250.08
----------------------------------------------------------------


2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace number of epoch : 100
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace number of pretraining epoch: 5
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace learning rate : 0.0001
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace learning rate milestone : [40, 80]
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace weight_decay : 1e-06
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace optimizer : Adam
2020-03-28 12:40:51,789 | INFO | Joint_DeepSVDD_Subspace batch_size 16
2020-03-28 12:40:51,790 | INFO | Joint_DeepSVDD_Subspace number of dataloader worker : 8
2020-03-28 12:40:51,790 | INFO | Joint_DeepSVDD_Subspace subspace projecttion distance metric : True
2020-03-28 12:40:51,790 | INFO | Joint_DeepSVDD_Subspace criterion weighting : 0.6 Reconstruction loss + 0.4 SVDD embdedding loss

2020-03-28 12:40:51,792 | INFO | >>> Start Pretraining the Autoencoder.
2020-03-28 12:47:37,373 | INFO | | Epoch: 001/005 | Pretrain Time: 405.580 [s] | Pretrain Loss: 0.008842 |
2020-03-28 12:54:29,258 | INFO | | Epoch: 002/005 | Pretrain Time: 411.884 [s] | Pretrain Loss: 0.004674 |
2020-03-28 13:01:21,696 | INFO | | Epoch: 003/005 | Pretrain Time: 412.438 [s] | Pretrain Loss: 0.003709 |
2020-03-28 13:08:14,487 | INFO | | Epoch: 004/005 | Pretrain Time: 412.791 [s] | Pretrain Loss: 0.003189 |
2020-03-28 13:15:07,385 | INFO | | Epoch: 005/005 | Pretrain Time: 412.897 [s] | Pretrain Loss: 0.002793 |
2020-03-28 13:15:07,386 | INFO | >>> Pretraining of AutoEncoder Time: 2055.594 [s]
2020-03-28 13:15:07,386 | INFO | >>> Finished of AutoEncoder Pretraining.

2020-03-28 13:15:07,393 | INFO | >>> Initializing the subspace projection matrix.
2020-03-28 13:16:18,253 | INFO | >>> Projection matrix succesfully initialized.
2020-03-28 13:16:18,253 | INFO | >>> Initializing the loss scale factors.
2020-03-28 13:18:38,279 | INFO | >>> reconstruction loss scale factor initialized to 395.194245
2020-03-28 13:18:38,279 | INFO | >>> SVDD embdeding loss scale factor initialized to 0.000007
2020-03-28 13:18:38,287 | INFO | >>> Start Training the Joint DeepSAD and Autoencoder.
2020-03-28 13:25:32,536 | INFO | | Epoch: 001/100 | Train Time: 414.248 [s] | Train Loss: 0.662562 |
2020-03-28 13:32:27,037 | INFO | | Epoch: 002/100 | Train Time: 414.501 [s] | Train Loss: 0.568133 |
2020-03-28 13:39:21,346 | INFO | | Epoch: 003/100 | Train Time: 414.308 [s] | Train Loss: 0.474603 |
2020-03-28 13:46:15,833 | INFO | | Epoch: 004/100 | Train Time: 414.486 [s] | Train Loss: 0.425618 |
2020-03-28 13:53:10,526 | INFO | | Epoch: 005/100 | Train Time: 414.693 [s] | Train Loss: 0.396574 |
2020-03-28 14:00:04,891 | INFO | | Epoch: 006/100 | Train Time: 414.365 [s] | Train Loss: 0.374905 |
2020-03-28 14:06:59,560 | INFO | | Epoch: 007/100 | Train Time: 414.668 [s] | Train Loss: 0.353954 |
2020-03-28 14:13:54,158 | INFO | | Epoch: 008/100 | Train Time: 414.598 [s] | Train Loss: 0.326128 |
2020-03-28 14:20:49,023 | INFO | | Epoch: 009/100 | Train Time: 414.865 [s] | Train Loss: 0.309568 |
2020-03-28 14:27:43,785 | INFO | | Epoch: 010/100 | Train Time: 414.761 [s] | Train Loss: 0.290514 |
2020-03-28 14:34:38,330 | INFO | | Epoch: 011/100 | Train Time: 414.545 [s] | Train Loss: 0.273609 |
2020-03-28 14:41:32,935 | INFO | | Epoch: 012/100 | Train Time: 414.605 [s] | Train Loss: 0.264633 |
2020-03-28 14:48:27,770 | INFO | | Epoch: 013/100 | Train Time: 414.834 [s] | Train Loss: 0.250911 |
2020-03-28 14:55:22,706 | INFO | | Epoch: 014/100 | Train Time: 414.935 [s] | Train Loss: 0.255531 |
2020-03-28 15:02:17,517 | INFO | | Epoch: 015/100 | Train Time: 414.810 [s] | Train Loss: 0.231883 |
2020-03-28 15:09:12,283 | INFO | | Epoch: 016/100 | Train Time: 414.766 [s] | Train Loss: 0.223072 |
2020-03-28 15:16:07,191 | INFO | | Epoch: 017/100 | Train Time: 414.907 [s] | Train Loss: 0.222689 |
2020-03-28 15:23:02,351 | INFO | | Epoch: 018/100 | Train Time: 415.159 [s] | Train Loss: 0.212142 |
2020-03-28 15:29:57,363 | INFO | | Epoch: 019/100 | Train Time: 415.012 [s] | Train Loss: 0.206421 |
2020-03-28 15:36:52,653 | INFO | | Epoch: 020/100 | Train Time: 415.290 [s] | Train Loss: 0.199921 |
2020-03-28 15:43:47,677 | INFO | | Epoch: 021/100 | Train Time: 415.023 [s] | Train Loss: 0.193010 |
2020-03-28 15:50:42,965 | INFO | | Epoch: 022/100 | Train Time: 415.287 [s] | Train Loss: 0.197251 |
2020-03-28 15:57:38,103 | INFO | | Epoch: 023/100 | Train Time: 415.138 [s] | Train Loss: 0.181673 |
2020-03-28 16:04:33,735 | INFO | | Epoch: 024/100 | Train Time: 415.631 [s] | Train Loss: 0.182054 |
2020-03-28 16:11:29,065 | INFO | | Epoch: 025/100 | Train Time: 415.329 [s] | Train Loss: 0.179149 |
2020-03-28 16:18:24,444 | INFO | | Epoch: 026/100 | Train Time: 415.378 [s] | Train Loss: 0.175380 |
2020-03-28 16:25:19,790 | INFO | | Epoch: 027/100 | Train Time: 415.345 [s] | Train Loss: 0.173596 |
2020-03-28 16:32:15,256 | INFO | | Epoch: 028/100 | Train Time: 415.466 [s] | Train Loss: 0.167390 |
2020-03-28 16:39:10,590 | INFO | | Epoch: 029/100 | Train Time: 415.333 [s] | Train Loss: 0.175338 |
2020-03-28 16:46:06,197 | INFO | | Epoch: 030/100 | Train Time: 415.606 [s] | Train Loss: 0.160173 |
2020-03-28 16:53:01,716 | INFO | | Epoch: 031/100 | Train Time: 415.519 [s] | Train Loss: 0.163356 |
2020-03-28 16:59:56,856 | INFO | | Epoch: 032/100 | Train Time: 415.139 [s] | Train Loss: 0.158542 |
2020-03-28 17:06:52,092 | INFO | | Epoch: 033/100 | Train Time: 415.236 [s] | Train Loss: 0.156963 |
2020-03-28 17:13:47,300 | INFO | | Epoch: 034/100 | Train Time: 415.207 [s] | Train Loss: 0.157308 |
2020-03-28 17:20:42,726 | INFO | | Epoch: 035/100 | Train Time: 415.425 [s] | Train Loss: 0.154655 |
2020-03-28 17:27:38,332 | INFO | | Epoch: 036/100 | Train Time: 415.605 [s] | Train Loss: 0.146541 |
2020-03-28 17:34:33,635 | INFO | | Epoch: 037/100 | Train Time: 415.303 [s] | Train Loss: 0.154958 |
2020-03-28 17:41:29,261 | INFO | | Epoch: 038/100 | Train Time: 415.626 [s] | Train Loss: 0.149081 |
2020-03-28 17:48:24,943 | INFO | | Epoch: 039/100 | Train Time: 415.680 [s] | Train Loss: 0.141880 |
2020-03-28 17:55:20,814 | INFO | | Epoch: 040/100 | Train Time: 415.871 [s] | Train Loss: 0.138017 |
2020-03-28 17:55:20,814 | INFO | >>> LR Scheduler : new learning rate 1e-06
2020-03-28 18:02:16,639 | INFO | | Epoch: 041/100 | Train Time: 415.825 [s] | Train Loss: 0.119894 |
2020-03-28 18:09:12,144 | INFO | | Epoch: 042/100 | Train Time: 415.505 [s] | Train Loss: 0.117910 |
2020-03-28 18:16:07,683 | INFO | | Epoch: 043/100 | Train Time: 415.538 [s] | Train Loss: 0.117211 |
2020-03-28 18:23:03,140 | INFO | | Epoch: 044/100 | Train Time: 415.456 [s] | Train Loss: 0.116544 |
2020-03-28 18:29:58,378 | INFO | | Epoch: 045/100 | Train Time: 415.237 [s] | Train Loss: 0.115571 |
2020-03-28 18:36:53,964 | INFO | | Epoch: 046/100 | Train Time: 415.586 [s] | Train Loss: 0.115296 |
2020-03-28 18:43:49,390 | INFO | | Epoch: 047/100 | Train Time: 415.425 [s] | Train Loss: 0.115411 |
2020-03-28 18:50:44,626 | INFO | | Epoch: 048/100 | Train Time: 415.236 [s] | Train Loss: 0.114026 |
2020-03-28 18:57:40,238 | INFO | | Epoch: 049/100 | Train Time: 415.611 [s] | Train Loss: 0.113556 |
2020-03-28 19:04:35,575 | INFO | | Epoch: 050/100 | Train Time: 415.337 [s] | Train Loss: 0.113694 |
2020-03-28 19:11:30,876 | INFO | | Epoch: 051/100 | Train Time: 415.300 [s] | Train Loss: 0.113049 |
2020-03-28 19:18:26,384 | INFO | | Epoch: 052/100 | Train Time: 415.507 [s] | Train Loss: 0.112635 |
2020-03-28 19:25:21,627 | INFO | | Epoch: 053/100 | Train Time: 415.243 [s] | Train Loss: 0.111916 |
2020-03-28 19:32:17,062 | INFO | | Epoch: 054/100 | Train Time: 415.435 [s] | Train Loss: 0.112129 |
2020-03-28 19:39:12,455 | INFO | | Epoch: 055/100 | Train Time: 415.392 [s] | Train Loss: 0.111737 |
2020-03-28 19:46:07,643 | INFO | | Epoch: 056/100 | Train Time: 415.188 [s] | Train Loss: 0.111072 |
2020-03-28 19:53:03,027 | INFO | | Epoch: 057/100 | Train Time: 415.383 [s] | Train Loss: 0.110564 |
2020-03-28 19:59:58,442 | INFO | | Epoch: 058/100 | Train Time: 415.414 [s] | Train Loss: 0.110815 |
2020-03-28 20:06:53,875 | INFO | | Epoch: 059/100 | Train Time: 415.432 [s] | Train Loss: 0.110436 |
2020-03-28 20:13:49,340 | INFO | | Epoch: 060/100 | Train Time: 415.465 [s] | Train Loss: 0.109683 |
2020-03-28 20:20:44,691 | INFO | | Epoch: 061/100 | Train Time: 415.351 [s] | Train Loss: 0.109005 |
2020-03-28 20:27:40,193 | INFO | | Epoch: 062/100 | Train Time: 415.501 [s] | Train Loss: 0.110165 |
2020-03-28 20:34:35,656 | INFO | | Epoch: 063/100 | Train Time: 415.463 [s] | Train Loss: 0.108878 |
2020-03-28 20:41:30,797 | INFO | | Epoch: 064/100 | Train Time: 415.140 [s] | Train Loss: 0.108552 |
2020-03-28 20:48:26,129 | INFO | | Epoch: 065/100 | Train Time: 415.331 [s] | Train Loss: 0.108247 |
2020-03-28 20:55:21,263 | INFO | | Epoch: 066/100 | Train Time: 415.134 [s] | Train Loss: 0.107634 |
2020-03-28 21:02:16,587 | INFO | | Epoch: 067/100 | Train Time: 415.323 [s] | Train Loss: 0.107581 |
2020-03-28 21:09:11,759 | INFO | | Epoch: 068/100 | Train Time: 415.172 [s] | Train Loss: 0.107711 |
2020-03-28 21:16:06,905 | INFO | | Epoch: 069/100 | Train Time: 415.145 [s] | Train Loss: 0.107093 |
2020-03-28 21:23:02,216 | INFO | | Epoch: 070/100 | Train Time: 415.311 [s] | Train Loss: 0.107157 |
2020-03-28 21:29:57,363 | INFO | | Epoch: 071/100 | Train Time: 415.146 [s] | Train Loss: 0.106601 |
2020-03-28 21:36:52,556 | INFO | | Epoch: 072/100 | Train Time: 415.193 [s] | Train Loss: 0.106113 |
2020-03-28 21:43:47,712 | INFO | | Epoch: 073/100 | Train Time: 415.155 [s] | Train Loss: 0.105809 |
2020-03-28 21:50:43,054 | INFO | | Epoch: 074/100 | Train Time: 415.342 [s] | Train Loss: 0.106171 |
2020-03-28 21:57:38,257 | INFO | | Epoch: 075/100 | Train Time: 415.202 [s] | Train Loss: 0.105821 |
2020-03-28 22:04:33,594 | INFO | | Epoch: 076/100 | Train Time: 415.337 [s] | Train Loss: 0.105383 |
2020-03-28 22:11:28,947 | INFO | | Epoch: 077/100 | Train Time: 415.352 [s] | Train Loss: 0.104565 |
2020-03-28 22:18:24,317 | INFO | | Epoch: 078/100 | Train Time: 415.370 [s] | Train Loss: 0.104981 |
2020-03-28 22:25:19,381 | INFO | | Epoch: 079/100 | Train Time: 415.062 [s] | Train Loss: 0.104882 |
2020-03-28 22:32:14,532 | INFO | | Epoch: 080/100 | Train Time: 415.151 [s] | Train Loss: 0.103743 |
2020-03-28 22:32:14,533 | INFO | >>> LR Scheduler : new learning rate 1e-07
2020-03-28 22:39:10,007 | INFO | | Epoch: 081/100 | Train Time: 415.474 [s] | Train Loss: 0.102798 |
2020-03-28 22:46:05,408 | INFO | | Epoch: 082/100 | Train Time: 415.400 [s] | Train Loss: 0.101989 |
2020-03-28 22:53:00,732 | INFO | | Epoch: 083/100 | Train Time: 415.324 [s] | Train Loss: 0.102528 |
2020-03-28 22:59:55,946 | INFO | | Epoch: 084/100 | Train Time: 415.212 [s] | Train Loss: 0.102130 |
2020-03-28 23:06:51,213 | INFO | | Epoch: 085/100 | Train Time: 415.267 [s] | Train Loss: 0.102360 |
2020-03-28 23:13:46,666 | INFO | | Epoch: 086/100 | Train Time: 415.453 [s] | Train Loss: 0.102223 |
2020-03-28 23:20:41,827 | INFO | | Epoch: 087/100 | Train Time: 415.161 [s] | Train Loss: 0.102312 |
2020-03-28 23:27:37,480 | INFO | | Epoch: 088/100 | Train Time: 415.652 [s] | Train Loss: 0.102374 |
2020-03-28 23:34:33,027 | INFO | | Epoch: 089/100 | Train Time: 415.547 [s] | Train Loss: 0.101821 |
2020-03-28 23:41:28,342 | INFO | | Epoch: 090/100 | Train Time: 415.314 [s] | Train Loss: 0.102315 |
2020-03-28 23:48:23,864 | INFO | | Epoch: 091/100 | Train Time: 415.522 [s] | Train Loss: 0.102240 |
2020-03-28 23:55:19,287 | INFO | | Epoch: 092/100 | Train Time: 415.422 [s] | Train Loss: 0.101976 |
2020-03-29 00:02:14,500 | INFO | | Epoch: 093/100 | Train Time: 415.213 [s] | Train Loss: 0.101874 |
2020-03-29 00:09:09,699 | INFO | | Epoch: 094/100 | Train Time: 415.198 [s] | Train Loss: 0.101880 |
2020-03-29 00:16:05,108 | INFO | | Epoch: 095/100 | Train Time: 415.409 [s] | Train Loss: 0.102049 |
2020-03-29 00:23:00,270 | INFO | | Epoch: 096/100 | Train Time: 415.162 [s] | Train Loss: 0.101603 |
2020-03-29 00:29:55,513 | INFO | | Epoch: 097/100 | Train Time: 415.242 [s] | Train Loss: 0.101637 |
2020-03-29 00:36:50,558 | INFO | | Epoch: 098/100 | Train Time: 415.044 [s] | Train Loss: 0.101862 |
2020-03-29 00:43:45,787 | INFO | | Epoch: 099/100 | Train Time: 415.228 [s] | Train Loss: 0.101910 |
2020-03-29 00:50:41,117 | INFO | | Epoch: 100/100 | Train Time: 415.330 [s] | Train Loss: 0.102072 |
2020-03-29 00:50:41,117 | INFO | >>> Training of Joint DeepSAD and AutoEncoder Time: 41522.831 [s]
2020-03-29 00:50:41,117 | INFO | >>> Finished Joint DeepSAD and AutoEncoder Training.

2020-03-29 00:50:41,131 | INFO | >>> Start Validating of the joint DeepSAD and AutoEncoder.
2020-03-29 00:51:53,957 | INFO | >>> Validation Time: 72.472 [s]
2020-03-29 00:51:53,957 | INFO | >>> Validation Loss: 0.100059
2020-03-29 00:51:53,957 | INFO | >>> Validation reconstruction AUC: 52.846%
2020-03-29 00:51:53,957 | INFO | >>> Best Threshold for the reconstruction score maximizing F1-score: 0.000
2020-03-29 00:51:53,958 | INFO | >>> Best F1-score on reconstruction score: 87.659%
2020-03-29 00:51:53,958 | INFO | >>> Validation DeepSAD AUC: 48.238%
2020-03-29 00:51:53,958 | INFO | >>> Best Threshold for the DeepSAD score maximizing F1-score: 43.171
2020-03-29 00:51:53,958 | INFO | >>> Best F1-score on DeepSAD score: 87.684%
2020-03-29 00:51:53,958 | INFO | >>> Finished validating the Joint DeepSAD and AutoEncoder.

2020-03-29 00:51:53,964 | INFO | >>> Start Testing the joint DeepSAD and AutoEncoder.
2020-03-29 00:53:08,053 | INFO | >>> Test Time: 74.033 [s]
2020-03-29 00:53:08,053 | INFO | >>> Test Loss: 0.101090
2020-03-29 00:53:08,053 | INFO | >>> Test reconstruction AUC: 54.074%
2020-03-29 00:53:08,053 | INFO | >>> Test F1-score on reconstruction score: 87.686%
2020-03-29 00:53:08,053 | INFO | >>> Test AD AUC: 50.407%
2020-03-29 00:53:08,053 | INFO | >>> Test F1-score on DeepSAD score: 87.716%
2020-03-29 00:53:08,053 | INFO | >>> Finished Testing the Joint DeepSAD and AutoEncoder.

2020-03-29 00:53:08,386 | INFO | Test results saved at ../../../Outputs/Joint_DeepSVDD_Subspace_2020_03_28_12h40/results/Joint_DeepSVDD_Subspace_results_1.json

2020-03-29 00:53:08,618 | INFO | Model saved at ../../../Outputs/Joint_DeepSVDD_Subspace_2020_03_28_12h40/model/Joint_DeepSVDD_Subspace_model_1.pt
